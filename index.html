<!DOCTYPE html>
<html lang="en">
  <head>
    <title>AlignBench</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://kit.fontawesome.com/f8ddf9854a.js" crossorigin="anonymous"></script>
    <meta charset="utf-8">
    <meta name="description"
          content="AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs">
    <meta name="keywords" content="AlignBench, VLM, VLM Evaluation, Vision Language Model, Large Language Model, Large Multimodal Model, artificial intelligence, AI, artificial general intelligence">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs</title>

    <link rel="icon" href="./static/images/agro_icon.png">

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <script src="https://kit.fontawesome.com/fff5b27ec1.js" crossorigin="anonymous"></script>
    <!-- <script src="https://kit.fontawesome.com/eaf1856e6f.js" crossorigin="anonymous"></script> -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
  </head>
  <body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <!-- <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link">
              More Research
            </a>
            <div class="navbar-dropdown">
              <a class="navbar-item" href="https://dahlian00.github.io/AnimalCluePage/">
                <b>AnimalClue</b> <span style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</span>
              </a>
              <a class="navbar-item" href="https://dahlian00.github.io/PetFacePage/">
                <b>PetFace</b> <span style="font-size:18px; display: inline; margin-left: 5px;">ðŸ”¥</span>
              </a>
              <a class="navbar-item" href="https://dahlian00.github.io/SegRCDBPage/">
                SegRCDB
              </a>
            
          </div>
        </div> -->
      </div>
    </nav>



    <!-- AFTERï¼ˆä¿®æ­£å¾Œï¼‰ -->
    <section class="hero">
      <div class="hero-body has-text-centered">
        <h1 class="title is-1 publication-title is-bold">
          <!-- <img src="static/images/agro_icon.png" style="width:1em; vertical-align: middle" alt="Logo"/> -->
          <span class="mmmu" style="vertical-align: middle">AlignBench</span>
        </h1>
        <h2 class="subtitle is-3 publication-subtitle">
          AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs
        </h2>
        <!-- <h3 class="subtitle is-3 publication-subtitle">
          a
        </h3> -->
        <div class="is-size-5 publication-authors">
          <br>
          <span class="author-block">
            <a href="https://ksaito-ut.github.io/" style="text-decoration: none; color: inherit;">Kuniaki Saito*<sup>1</sup></a>,
          </span>
          <span class="author-block">
            <a href="https://sites.google.com/view/risashinoda/home" style="text-decoration: none; color: inherit;">Risa Shinoda*<sup>2</sup></a>,
          </span>
          <span class="author-block">
            <a href="https://shohei-ta-ds7.github.io/" style="text-decoration: none; color: inherit;">Shohei Tanaka<sup>1</sup></a>,
          </span>
          <span class="author-block">
            <a href="https://toshohirasawa.github.io/" style="text-decoration: none; color: inherit;">Tosho Hirasawa<sup>1</sup></a>,
          </span>
          <span class="author-block">
            <a href="https://fokura.jp/" style="text-decoration: none; color: inherit;">Fumio Okura<sup>2</sup></a>,
          </span>
          <span class="author-block">
            <a href="https://yoshitakaushiku.net/index.html" style="text-decoration: none; color: inherit;">Yoshitaka Ushiku<sup>1</sup></a>
          </span>
        </div>
        <div class="is-size-6 has-text-centered mt-3">
          <p><sup>1</sup>OMRON SINIC X <sup>2</sup>The University of Osaka</p>
          <p><sup>*</sup>Equal contribution. Kuniaki serves as the project lead, while Risa isresponsible for dataset construction.</p> 
        </div>
      </div>
      <div class="column has-text-centered">
        <div class="publication-links">
          <span class="link-block">
            <a href="https://arxiv.org/abs/2511.20515" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fas fa-file-pdf"></i>
              </span>
              <span>arXiv</span>
            </a>
          </span>
          <span class="link-block">
            <a href="https://huggingface.co/datasets/omron-sinicx/AlignBench" class="external-link button is-normal is-rounded is-dark">
              <span class="icon" style="font-size:18px">ðŸ¤—</span>
              <span>Dataset</span>
            </a>
          </span>
          <!-- <span class="link-block">
            <a href="https://github.com/dahlian00/AgroBench/tree/main" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span> -->
        </div>
      </div>
    </section>


    <section class="hero teaser">
      <div class="container is-max-desktop">
        <img src="static/images/teaser_v6.png" alt="geometric reasoning",style="margin-top: 20rem;">
        <p>We introduce a novel benchmark, AlignBench, which evaluates the VLM's ability for text-image alignment. We employ state-of-the-art Image-to-Text and Text-to-Image models to create synthetic image-caption pairs with or without subtle hallucinations. Misaligned words are highlighted in red. Using this dataset, we benchmark diverse VLMs to assess their ability to understand the alignment of image-sentence pairs. We find that subtle hallucinations generated by multimodal models can be hard to detect, even by state-of-the-art VLMs. 
        </p>
      </div>
    </section> 

    <section class="section">
      <div class="container" style="margin-bottom: 2vh;">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">ðŸ””News</h2>
            <div class="content has-text-justified">
              <p>
                ðŸ”¥[2025-11-26] Introducing AlignBench, which evaluates the VLM's ability for text-image alignment ! ðŸš€
              </p>
          </div>
            <h2 class="title is-3">Abstruct</h2>
            <div class="content has-text-justified">
              <p>
                Assessing imageâ€“text alignment models such as CLIP is crucial for bridging visual and linguistic representations. 
                Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. 
                We introduce AlignBench, a benchmark that provides a new indicator of imageâ€“text alignment by evaluating detailed imageâ€“caption pairs generated by diverse image-to-text and text-to-image models. 
                Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. 
                Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance.
              </p>
            </div>
          </div>
        </div>
    
    </div>
    </section>

    <!-- DATASET SECTION -->
    <!-- <section class="hero is-light is-small">
      <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">
          <img src="static/images/agro_icon.png" alt="Logo" class="mmmu-logo"/>
          <span class="mmmu">AnimalClue</span>
        </h1>
      </div>
    </section> -->
    <section class="hero is-light is-small">
      <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">
          <!-- <img src="static/images/agro_icon.png" alt="Logo" class="mmmu-logo"/> -->
          <span class="mmmu">AlignBench</span>
        </h1>
      </div>
    </section>
    
    


    <section class="section">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Example</h2>
            <div class="content has-text-justified">
              <img src="static/images/ex_annotation_picture.png" alt="ex_annotation" class="center">
              <br>
              <p>
                <b>Examples of hallucinated sentences in AlignBench.</b>
                The hallucinated portions are often subtle,requiring fine-grained image-text alignment ability to detect them.
              </p>
            </div>
          </div>
        </div>
    

        <div class="columns is-centered has-text-centered"  style="margin-top: 3em;">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Stats of AlignBench</h2>
            <div class="content has-text-justified">
              <!-- <p>
                AgroBench includes multiple topics with a diverse range of categories. The total accuracy is calculated by the average of each task to mitigate the difference in QAs.
              </p> -->
                <img src="static/images/stats_table.png" alt="algebraic reasoning" class="center" style="width: 80%;">
                <p>
                  <b>Stats of AlignBench in sentence-level correctness annotations. </b> AlignBench contains a large number of annotated sentences, enough for benchmarking models. We exclude sentences with unknown label.
                </p>
            </div>
          </div>
        </div>
        <div class="columns is-centered has-text-centered"  style="margin-top: 3em;">
          <div class="column is-four-fifths">
            <!-- <h2 class="title is-3">Example Annotations</h2> -->
            <div class="content has-text-justified">
                <img src="static/images/stats_figure.png" alt="algebraic reasoning" class="center">
                <p> <b>Left: Ratio of incorrect sentences by position</b>; all captioners make fewer errors at the first position. Different colors indicate different positions.  <b>Right: Number of unaligned sentences per category</b>; most mistakes occur in attributes and text.</p>
            </div>
          </div>
        </div>

      </div>
    </section>

    <!-- RESULTS SECTION -->
    <section class="hero is-light is-small">
      <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">Overview of Results</h1>
      </div>
    </section>

    <section class="section">
      <div class="container">
    
        <!-- RESULTS -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Main Results</h2>
            <div class="content has-text-justified">
              <img src="static/images/AUROC.png" alt="algebraic reasoning" class="center">
              <p>
                <b>AUROC results across VLMs.</b> Cells with the best performance within open-source and closed-source groups are highlighted in a blue background, while the best model within each model family is marked in bold.
              </p>
            </div>
            <div style="max-width: 80%; margin: 2em auto; padding: 1em 1.2em; border-left: 4px solid #ff8484; background: #fff8f8; border-radius: 6px;">
              <p style="margin: 0 0 .6em; font-weight: 600; text-align: left;">
                <i class="fa-solid fa-pen-to-square" style="color:#ff8484; margin-right: .4em;"></i>
                Key Points
              </p>
              <ul style="margin: 0; padding-left: 1.4em; text-align: left; line-height: 1.6; list-style-type: disc;">
                <li>AlignBench covers diverse levels of hallucination detection.</li>
                <li>CLIP-based models tailored for compositional alignment remain nearly blind.  </li>
                <li>GPT-5 shows the best performance of all models, while Llama-4, the best open-source model.</li>
                <li>Advanced captioners produces hard-to-detect errors.</li>
                <li>Increasing the model size improves performance.</li>
                <li>Robustness to text-to-image models differs by VLMs.</li>
              </ul>
            </div>
            
          
          </div>
        </div>


    

      </div>

      <section class="hero is-light is-small">
        <div class="hero-body has-text-centered">
          <h1 class="title is-1 mmmu">Analysys</h1>
        </div>
      </section>
      <section class="section">
        <div class="container">
      
          <!-- RESULTS -->
          <div class="columns is-centered has-text-centered"  style="margin-top: 3em;">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Correlation with MMMU performance</h2>
              <div class="content has-text-justified">
                <img src="static/images/mmmu.png" alt="algebraic reasoning" class="center">
                <p>
                The size of plots indicates the parameter size.<b> Left: MMMU performance measured on Captioners (X-axis) vs. AUROC measured by GPT-5-mini (Y-axis) for each Captioner. </b> Advanced Captioners tend to produce hard-to-detect hallucinations. <b> Right: MMMU (X-axis) vs. AUROC (Y-axis) measured on each detector.</b>  Detectors with better MMMU performance tend to perform better on AlignBench.
                </p>
              </div>
              <div style="max-width: 80%; margin: 2em auto; padding: 1em 1.2em; border-left: 4px solid #ff8484; background: #fff8f8; border-radius: 6px;">
                <p style="margin: 0 0 .6em; font-weight: 600; text-align: left;">
                  <i class="fa-solid fa-pen-to-square" style="color:#ff8484; margin-right: .4em;"></i>
                  Key Points
                </p>
                <ul style="margin: 0; padding-left: 1.4em; text-align: left; line-height: 1.6; list-style-type: disc;">
                  <li>Stronger Captioners generate hallucinations that are harder to detect.</li>
                  <li>The performance on HalDec-Bench is highly correlated with that on MMMU.</li>
                </ul>
              </div>
              
          
            </div>
          </div>
      
          <div class="columns is-centered has-text-centered"  style="margin-top: 3em;">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Positional bias</h2>
              <div class="content has-text-justified">
                <img src="static/images/position.png" alt="algebraic reasoning" class="center">
                <p>
                  <b>Detectors show positional bias in scoring.</b> We average the detectorsâ€™ correctness scores (Y-axis) by sentence position (X-axis) and visualize the results using GPT-4o (Left) and Llama-4 (Right) as detectors. Both detectors assign higher scores to sentences appearing near the beginning of the output. The detector is not provided with any positional information during inference.
                </p>
              </div>
              <div style="max-width: 80%; margin: 2em auto; padding: 1em 1.2em; border-left: 4px solid #ff8484; background: #fff8f8; border-radius: 6px;">
                <p style="margin: 0 0 .6em; font-weight: 600; text-align: left;">
                  <i class="fa-solid fa-pen-to-square" style="color:#ff8484; margin-right: .4em;"></i>
                  Key Points
                </p>
                <ul style="margin: 0; padding-left: 1.4em; text-align: left; line-height: 1.6; list-style-type: disc;">
                  <li>The detectors give a higher score to the sentences located near the beginning of the output.</li>
                </ul>
              </div>
              
          
            </div>
          </div>
          <!-- <div class="columns is-centered has-text-centered"  style="margin-top: 3em;">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Comparison to existing datasets</h2>
              <div class="content has-text-justified">
                <img src="static/images/existing.png" alt="algebraic reasoning" class="center">
                <p>
                  <b>Comparison to existing datasets (AUROC). </b>We compare with other HalDec datasets and the dataset used to assess VLMâ€™s compositionality understanding. Numbers of prior benchmarks that exceed HalDecBench are highlighted. This result indicates that HalDec-Bench is more challenging than existing datasets.
                </p>
              </div>
              <div style="max-width: 80%; margin: 2em auto; padding: 1em 1.2em; border-left: 4px solid #ff8484; background: #fff8f8; border-radius: 6px;">
                <p style="margin: 0 0 .6em; font-weight: 600; text-align: left;">
                  <i class="fa-solid fa-pen-to-square" style="color:#ff8484; margin-right: .4em;"></i>
                  Key Points
                </p>
                <ul style="margin: 0; padding-left: 1.4em; text-align: left; line-height: 1.6; list-style-type: disc;">
                  <li>ShareGPT is the easiest split of HalDec-Bench, with performance close to HAT. </li>
                  <li>All detectors excel on FOIL, suggesting these hallucinations are easy to detect for current SOTA VLMs.</li>
                </ul>
              </div>
              
          
            </div>
          </div> -->

          <div class="columns is-centered has-text-centered"  style="margin-top: 3em;">
            <div class="column is-four-fifths">
              <h2 class="title is-3">For own hallucination</h2>
              <div class="content has-text-justified">
                <img src="static/images/self_pref.png" alt="algebraic reasoning" class="center">
                <p>
                  Detectors struggle to detect their own hallucination. <b>Left: Self- and cross-evaluation results. AUROC scores for each Captioner (columns), normalized by the average AUROC of each Detector (rows).</b> Diagonal entries show self-evaluation. <b>Right: We pick GPT-4o as a detector, with their output correctness scores averaged by sentence position. </b>Blue and red lines show scores for correct and incorrect GPT-4oâ€™s outputs; green shows scores for incorrect Llama-4 outputs.
                </p>
              </div>
              <div style="max-width: 80%; margin: 2em auto; padding: 1em 1.2em; border-left: 4px solid #ff8484; background: #fff8f8; border-radius: 6px;">
                <p style="margin: 0 0 .6em; font-weight: 600; text-align: left;">
                  <i class="fa-solid fa-pen-to-square" style="color:#ff8484; margin-right: .4em;"></i>
                  Key Points
                </p>
                <ul style="margin: 0; padding-left: 1.4em; text-align: left; line-height: 1.6; list-style-type: disc;">
                  <li>Detectors struggle to detect their own hallucinations.  </li>
                </ul>
              </div>
              
          
            </div>
          </div>
<!-- 
          <div class="columns is-centered has-text-centered"  style="margin-top: 3em;">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Image domain and hullucination type</h2>
              <div class="content has-text-justified">
                <img src="static/images/domain.png" alt="algebraic reasoning" class="center">
                <p>
                  <b>Left: AUROC on different image domains.</b> The relative ordering of AUROC was highly consistent across models, exhibiting similar patterns of strength and weakness across domains.  <b>Right: Detectorsâ€™ score averaged within each hallucination type.</b> All models show weakness in Direction and Number hallucination.
                </p>
              </div>
              <div style="max-width: 80%; margin: 2em auto; padding: 1em 1.2em; border-left: 4px solid #ff8484; background: #fff8f8; border-radius: 6px;">
                <p style="margin: 0 0 .6em; font-weight: 600; text-align: left;">
                  <i class="fa-solid fa-pen-to-square" style="color:#ff8484; margin-right: .4em;"></i>
                  Key Points
                </p>
                <ul style="margin: 0; padding-left: 1.4em; text-align: left; line-height: 1.6; list-style-type: disc;">
                  <li>Detectors show similar domain-wise robustness.</li>
                  <li>Performance is notably lower on Nature, Food, and Object. </li>
                  <li>Detectors are poor at detecting Direction and Number hallucinations.</li>
                </ul>
              </div>
              
          
            </div>
          </div> -->

          <!-- <div class="columns is-centered has-text-centered" style="margin-top: 3em;">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Segment localization task </h2>
              <div class="content has-text-justified">
                <img src="static/images/segment.png" alt="algebraic reasoning" class="center">
                <p>
                  <b>Results of hallucinated segment localization task (Average precision (%)).</b>  Localizing the segment of the hallucinated caption is challenging even for performant models.
                </p>
              </div>
              <div style="max-width: 80%; margin: 2em auto; padding: 1em 1.2em; border-left: 4px solid #ff8484; background: #fff8f8; border-radius: 6px;">
                <p style="margin: 0 0 .6em; font-weight: 600; text-align: left;">
                  <i class="fa-solid fa-pen-to-square" style="color:#ff8484; margin-right: .4em;"></i>
                  Key Points
                </p>
                <ul style="margin: 0; padding-left: 1.4em; text-align: left; line-height: 1.6; list-style-type: disc;">
                  <li>Segment-level localization has more room for improvement. </li>
                  <li>Strong sentence-level detectors are not always effective for segment-level localization. </li>
                </ul>
              </div>
            </div>
          </div> -->

          <div class="columns is-centered has-text-centered" style="margin-top: 3em;">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Model ensemble </h2>
              <div class="content has-text-justified">
                <img src="static/images/ensemble.png" alt="algebraic reasoning" class="center">
                <p>
                  <b>Results of model ensemble.</b> Ensembling detectorsâ€™ outputs improves performance in almost all cases. The increase or decrease from the better model used for ensembling is highlighted next to each score.
                </p>
              </div>
              <div style="max-width: 80%; margin: 2em auto; padding: 1em 1.2em; border-left: 4px solid #ff8484; background: #fff8f8; border-radius: 6px;">
                <p style="margin: 0 0 .6em; font-weight: 600; text-align: left;">
                  <i class="fa-solid fa-pen-to-square" style="color:#ff8484; margin-right: .4em;"></i>
                  Key Points
                </p>
                <ul style="margin: 0; padding-left: 1.4em; text-align: left; line-height: 1.6; list-style-type: disc;">
                  <li>Ensembling improves performance.</li>
                  <li>Strong sentence-level detectors are not always effective for segment-level localization. </li>
                </ul>
              </div>
            </div>
          </div>
          <!-- ANALYSIS -->
          <!-- <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Analysis</h2>
              <div class="content has-text-justified">
                <img src="static/images/parameter_size.png" alt="algebraic reasoning" class="center">
                <p>
                  The size of plots indicates the parameter size. Left: MMMU performance measured on Captioners (X-axis) vs. AUROC measured by GPT-5-mini (Y-axis) for each Captioner. Advanced Captioners tend to produce hard-to-detect hallucinations. Right: MMMU (X-axis) vs. AUROC (Y-axis) measured on each detector. Detectors with better MMMU performance tend to show better performance on our benchmark.
                </p>
              </div>
      
       -->
        </div>
  
        <!-------------------------------------------------------------------- Error Example  -------------------------------------------------------------------->
        <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
            <h2 class="title is-3" id="examples">Dataset Examples</h2>
            <div class="carousel results-carousel">
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/examples/sharegpt.png" alt="grade-lv" width="60%"/>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/examples/llava.png" alt="grade-lv" width="60%"/>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/examples/qwen2.png" alt="grade-lv" width="60%"/>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/examples/gpt4o.png" alt="grade-lv" width="55%"/>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/examples/cogvlm.png" alt="grade-lv" width="55%"/>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/examples/llama-4.png" alt="grade-lv" width="60%"/>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/examples/sd.png" alt="grade-lv" width="60%"/>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/examples/gpt-gen.png" alt="grade-lv" width="60%"/>
                </div>
              </div>

    </section>

    <!-- @PAN TODO: bibtex -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title is-3 has-text-centered">BibTeX</h2>
        <pre><code>
          @article{saito2025alignbenchbenchmarkingfinegrainedimagetext,
            title={AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs}, 
            author={Kuniaki Saito and Risa Shinoda and Shohei Tanaka and Tosho Hirasawa and Fumio Okura and Yoshitaka Ushiku},
            year={2025},
            eprint={2511.20515},
            archivePrefix={arXiv},
            primaryClass={cs.CV},
            url={https://arxiv.org/abs/2511.20515}, 
          }
    </code></pre>
      </div>
    </section>

    <footer class="footer">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://mathvista.github.io/">MathVista</a>, licensed under a <a rel="license"
                                                  href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </footer>

  </body>
</html>
